import json
from litellm import completion
from text2sql.config import Config
from text2sql.constants import BENCHMARK

def generate_db_description(db_name: str, table_names: list, temperature: float = 0.5,
                                      max_tokens: int = 150) -> str:
    """
    Generates a short and concise description of a database.

    Args:
    - database_name (str): The name of the database.
    - table_names (list): A list of table names within the database.
    - temperature (float): Sampling temperature. Higher values mean more creativity (default: 0.5).
    - max_tokens (int): The maximum number of tokens to generate in the response (default: 150).

    Returns:
    - str: A description of the database.
    """
    if not isinstance(db_name, str):
        raise TypeError("database_name must be a string.")
    
    if not isinstance(table_names, list):
        raise TypeError("table_names must be a list of strings.")
    
    # Ensure all table names are strings
    if not all(isinstance(table, str) for table in table_names):
        raise ValueError("All items in table_names must be strings.")
    
    if not table_names:
        return f"{db_name} is a database with no tables."

    # Format the table names appropriately
    if len(table_names) == 1:
        tables_formatted = table_names[0]
    elif len(table_names) == 2:
        tables_formatted = " and ".join(table_names)
    else:
        tables_formatted = ", ".join(table_names[:-1]) + f", and {table_names[-1]}"

    prompt = (
        f"You are a helpful assistant. Generate a concise description of the database named '{db_name}' "
        f"which contains the following tables: {tables_formatted}.\n\n"
        f"Description:"
    )
    
    response = completion(model=Config.COMPLETION_MODEL,
                                messages=[
                {"role": "system", "content": "You are a helpful assistant."},
                {"role": "user", "content": prompt}
            ],
            temperature=temperature,
            max_tokens=max_tokens,
            n=1,
            stop=None
        )
    description = response.choices[0].message['content'].strip()
    return description

def llm_answer_validator(question: str, answer: str, expected_answer: str) -> float:
    prompt = """
    You are evaluating an answer generated by a text-to-sql RAG-based system. Assess how well the Generated Answer (generated sql) addresses the Question
    based on the Expected Answer.
    
    Question:
    {question}
    
    Expected Answer:
    {expected_answer}
    
    Generated Answer:
    {generated_answer}
    
    Provide a relevance score from 0 to 1 (1 being a perfect response) and justify your reasoning in a concise explanation.
    Output Json format:
    {{"relevance_score": float, "explanation": "Your assessment here."}}
    """
    response = completion(model=Config.VALIDTOR_MODEL,
                            messages=[
                                {"role": "system", "content": "You are a Validator assistant."},
                                {"role": "user", "content": prompt.format(question=question, expected_answer=expected_answer, generated_answer=answer)}
                            ],
                            response_format={"type": "json_object"},

                        )
    validation_set = response.choices[0].message['content'].strip()
    return validation_set


def llm_answer_validator(question: str, answer: str, expected_answer: str=None) -> float:
    prompt = """
    You are evaluating an answer generated by a text-to-sql RAG-based system. Assess how well the Generated Answer (generated sql) addresses the Question
    based on the Expected Answer.
    
    Question:
    {question}
    
    Expected Answer:
    {expected_answer}
    
    Generated Answer:
    {generated_answer}
    
    Provide a relevance score from 0 to 1 (1 being a perfect response) and justify your reasoning in a concise explanation.
    Output Json format:
    {{"relevance_score": float, "explanation": "Your assessment here."}}
    """
    response = completion(model=Config.VALIDTOR_MODEL,
                            messages=[
                                {"role": "system", "content": "You are a Validator assistant."},
                                {"role": "user", "content": prompt.format(question=question, expected_answer=expected_answer, generated_answer=answer)}
                            ],
                            response_format={"type": "json_object"},

                        )
    validation_set = response.choices[0].message['content'].strip()
    return validation_set

def llm_table_validator(question: str, answer: str, tables: list[str]) -> float:
    prompt = """
    You are evaluating an answer generated by a text-to-sql RAG-based system. Assess how well the retrived Tables relevant to the question and supports the Generated Answer (generated sql).
    - The tables are with the following structure:
    {{"schema": [["table_name", description, [{{"column_name": "column_description", "data_type": "data_type",...}},...]],...]}}

    Question:
    {question}
    
    Tables:
    {tables}
    
    Generated Answer:
    {generated_answer}
    
    Provide a relevance score from 0 to 1 (1 being a perfect response) and justify your reasoning in a concise explanation.
    Output Json format:
    {{"relevance_score": float, "explanation": "Your assessment here."}}
    """
    response = completion(model=Config.VALIDTOR_MODEL,
                            messages=[
                                {"role": "system", "content": "You are a Validator assistant."},
                                {"role": "user", "content": prompt.format(question=question, tables=tables, generated_answer=answer)}
                            ],
                            response_format={"type": "json_object"},

                        )
    validation_set = response.choices[0].message['content'].strip()
    try:
        val_res = json.loads(validation_set)
        score = val_res['relevance_score']
        explanation = val_res['explanation']
    except Exception as e:
        print(f"Error: {e}")
        score = 0.0
        explanation = "Error: Unable to parse the response."

    return score, explanation


def run_benchmark():
    """
    Run the benchmark for the text2sql module.
    """
    # Load the benchmark data
    benchmark_data = BENCHMARK

    # Initialize the benchmark results
    results = []

    for data in benchmark_data:
        success, result = generate_db_description(
            db_name=data['database'],
            table_names=list(data['tables'].keys())
        )

        if success:
            results.append(result)
        else:
            results.append(f"Error: {result}")

    return results